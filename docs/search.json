[
  {
    "objectID": "C00_coding.html",
    "href": "C00_coding.html",
    "title": "Coding",
    "section": "",
    "text": "Coding is the practice of writing instructions for computers to follow; computers aren’t clever by themselves - they need to be told what to do. Most coding is text-based; people writing coding instructions into simple text documents. But some coding is graphical or visual. We shall be using text-based coding. We are going to use a free and open source general programming language called R. R programming language has its roots in statistics and science, but it really can be used for anything.\nIn the early days, coding was pretty barebones - all one needed was a text editor and access to the programming language - no frills there, no pretty images, no buttons to push, just typing. As time passed, volunteers added niceties to the text editor, like visualizing plots of data, buttons to save files, colorized text for the typed code, and other bells and whistles. These editors became know as graphical user interfaces (GUI for short.) GUIs keep getting easier and easier for people to use. We will use the GUI known as RStudio. It’s best to think of GUIs as wrappers around the core programming language; they are really nice and pretty, but they can’t do math. The programming language itself (which does do math!), evolved only as it needed to to fix bugs and make general improvements.",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C00_coding.html#loading-the-necessary-tools",
    "href": "C00_coding.html#loading-the-necessary-tools",
    "title": "Coding",
    "section": "4.1 Loading the necessary tools",
    "text": "4.1 Loading the necessary tools\nFor any coding project you will need to access a select number of tools, often stored on your computer in what is called a package library (it’s just a directory/folder really). When the package is loaded from the library, all of the functionality the author built in to that package is exposed for you to use in your project. We have created a single file that will both install (if needed) and load (if not already loaded) each of these packages. It’s easy to run.\nFirst, make sure that you have loaded the project (File &gt; Open Project) if you haven’t already. Then at the R console pane type the following…\n\nsource(\"setup.R\")\n\nAfter a few moments the command prompt will return to focus. Be sure to run that command at the beginning of every new R session or anytime you are adding new functionality.\nNow we are ready to load some data into your R session.",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C00_coding.html#spatial-data",
    "href": "C00_coding.html#spatial-data",
    "title": "Coding",
    "section": "4.2 Spatial data",
    "text": "4.2 Spatial data\nSpatial data is any data that has been assigned to a location on a planet (or even between planets!); that means environmental data is mapped to locations on oblate spheroids (like Earth). The oblate spheroid shape presents interesting but challenging math to the data scientist. Modern spatial data is designed to make data science easier by handling all of the location information in a discrete and standardized manner. By discrete we mean that we don’t have to sweat the details.\n\n4.2.1 Point data\nMany spatial data sets come as point data - locations (longitude, latitude and maybe altitude/depth and/or time) with one or more measurements (temperature, cloudiness, probability of precipitation, abundance of fish, population density, etc) attached to that point. Here is an example of point data about long-term oceanographic monitoring buoys in the Gulf of Maine (“gom”). We’ll read the buoy data into a variable, buoy. Next we can print the result simply by typing the name (or you could type print(buoys) if you like all the extra typing.)\n\nbuoys = gom_buoys()\nbuoys\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -70.4277 ymin: 42.3233 xmax: -65.9267 ymax: 44.10163\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  name  longname            id                geometry\n* &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;POINT [°]&gt;\n1 wms   Western Maine Shelf B01    (-70.4277 43.18065)\n2 cms   Central Maine Shelf E01     (-69.3578 43.7148)\n3 pb    Penobscot Bay       F01   (-68.99689 44.05495)\n4 ems   Eastern Maine Shelf I01   (-68.11359 44.10163)\n5 jb    Jordan Basin        M01   (-67.88029 43.49041)\n6 nec   Northeast Channel   N01     (-65.9267 42.3233)\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can get the online documention for functions a couple of ways. You can type ?name_of_function, or or help(name_of_function). Try ?gom_buoys as an example.\nSometimes you need more - like seeing the function itself. You can always try typing the function name without any trailing parentheses.\n\ngom_buoys\n\nfunction (form = c(\"table\", \"sf\")[2]) \n{\n    x = structure(list(name = c(\"wms\", \"cms\", \"pb\", \"ems\", \"jb\", \n        \"nec\"), longname = c(\"Western Maine Shelf\", \"Central Maine Shelf\", \n        \"Penobscot Bay\", \"Eastern Maine Shelf\", \"Jordan Basin\", \n        \"Northeast Channel\"), id = c(\"B01\", \"E01\", \"F01\", \"I01\", \n        \"M01\", \"N01\"), lon = c(-70.4277, -69.3578, -68.99689, \n        -68.11359, -67.88029, -65.9267), lat = c(43.18065, 43.7148, \n        44.05495, 44.10163, 43.49041, 42.3233)), row.names = c(NA, \n        -6L), class = c(\"tbl_df\", \"tbl\", \"data.frame\"))\n    if (tolower(form[1]) == \"sf\") \n        x = sf::st_as_sf(x, coords = c(\"lon\", \"lat\"), crs = 4326)\n    x\n}\n&lt;bytecode: 0x564495dcf1a8&gt;\n\n\nIf that still doesn’t work, we highly recommend trying Rseek.org which is an R-language specific search engine.\n\n\nSo there are 6 buoys, each with an attached attribute “name”, “longname” and “id”, as well as the spatial location datain the “geometry” column (just longitude and latitude in this case). We can easily plot these using the “name” column as a color key. For more on plotting spatial data, see this wiki page.\n\nplot(buoys['id'], axes = TRUE, pch = 16)\n\n\n\n\n\n\n\n\nWell, that’s pretty, but without a shoreline it lacks context.\n\n\n4.2.2 Linestrings and polygon data\nLinestrings (open shapes) and polygons (closed shape) are much like point data, except that each geometry is linestring or polygon. We have a set of polygons/linestring that represent the coastline.\n\ncoast = read_coastline()\ncoast\n\nSimple feature collection with 14 features and 0 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -74.9 ymin: 38.95218 xmax: -65 ymax: 46.06477\nGeodetic CRS:  WGS 84\n# A tibble: 14 × 1\n                                                                            geom\n                                                           &lt;MULTILINESTRING [°]&gt;\n 1 ((-72.1019 41.01504, -72.15127 41.05146, -72.18389 41.04678, -72.28745 41.02…\n 2 ((-73.68745 45.56143, -73.85293 45.51572, -73.96055 45.44141, -73.92021 45.4…\n 3 ((-73.69531 45.5855, -73.57236 45.69448, -73.72466 45.67183, -73.85771 45.57…\n 4 ((-66.32412 44.25732, -66.27378 44.29229, -66.21035 44.39204, -66.25049 44.3…\n 5 ((-68.69077 44.24873, -68.70303 44.23198, -68.70171 44.18267, -68.66118 44.1…\n 6 ((-66.89707 44.62891, -66.7625 44.68179, -66.75337 44.70981, -66.74541 44.79…\n 7 ((-68.29941 44.45649, -68.34702 44.43037, -68.40947 44.36426, -68.41172 44.2…\n 8 ((-71.39307 41.46675, -71.36533 41.48525, -71.35449 41.54229, -71.36431 41.5…\n 9 ((-74.25049 39.52939, -74.1332 39.68076, -74.10674 39.74644, -74.25317 39.55…\n10 ((-74.18818 40.6146, -74.23589 40.5187, -74.18813 40.52285, -74.13853 40.541…\n11 ((-70.67373 41.44854, -70.7605 41.37358, -70.8292 41.35898, -70.7853 41.3274…\n12 ((-71.34624 41.46938, -71.29092 41.4646, -71.24141 41.49194, -71.23203 41.65…\n13 ((-70.0627 41.32847, -70.08662 41.31758, -70.23306 41.28633, -70.05508 41.24…\n14 ((-74.9 39.14709, -74.89702 39.14546, -74.9 39.1329), (-74.9 38.95218, -74.7…\n\n\nIn this case, each record of geometry is a “MULTILINESTRING”, which is a group of one or more linestrings. Note that no other variables are in this table - it’s just the geometry.\nLet’s plot these geometries, and add the points on top.\n\nplot(coast, col = \"orange\", lwd = 2, axes = TRUE, reset = FALSE,\n     main = \"Buoys in the Gulf of Maine\")\nplot(st_geometry(buoys), pch = 1, cex = 0.5, add = TRUE)\ntext(st_geometry(buoys), labels = buoys$id, cex = 0.7, adj = c(1,-0.1))\n\n\n\n\n\n\n\n\n\n\n4.2.3 Array data (aka raster data)\nOften spatial data comes in grids, like regular arrays of pixels. These are great for all sorts of data like satellite images, bathymetry maps and environmental modeling data. We’ll be working with environmental modeling data which we call “Brickman data”. You can learn more about Brickman data in the wiki. We’ll be glossing over the details here, but there’s lots of detail in the wiki.\nWe’ll read in the database that tracks 82 Brickman data files, and then immediately filter out the rows that define the “PRESENT” scenario (where present means 1982–2013) and monthly climatology models.\n\ndb = brickman_database() |&gt;\n  filter(scenario == \"PRESENT\", interval == \"mon\") # note the double '==', it's comparative\ndb\n\n# A tibble: 8 × 4\n  scenario year    interval var  \n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;\n1 PRESENT  PRESENT mon      MLD  \n2 PRESENT  PRESENT mon      Sbtm \n3 PRESENT  PRESENT mon      SSS  \n4 PRESENT  PRESENT mon      SST  \n5 PRESENT  PRESENT mon      Tbtm \n6 PRESENT  PRESENT mon      U    \n7 PRESENT  PRESENT mon      V    \n8 PRESENT  PRESENT mon      Xbtm \n\n\nIf you are wondering about filtering a table, be sure to check out the wiki on tabular data to get started.\nYou might be wondering what that |&gt; is doing. It is called a pipe, and it delivers the output of one function to the next function as the first parameter (aka argument). For example, brickman_database() produces a table, that table is immediately passed into filter() to choose rows that match our criteria.\nNow that we have the database listing just the records we want, we pass it to the read_brickman() function.\n\ncurrent = read_brickman(db)\ncurrent\n\nstars object with 3 dimensions and 9 attributes\nattribute(s):\n                Min.      1st Qu.        Median          Mean      3rd Qu.\ndepth   5.000000e+00 60.258880615 145.012619019 923.313763739 1.704049e+03\nMLD     1.011275e+00  5.583339810  15.967359543  18.910421492 2.809953e+01\nSbtm    2.324167e+01 32.136343956  34.232215881  33.507147254 3.491243e+01\nSSS     1.644333e+01 30.735633373  31.104771614  31.492407921 3.203519e+01\nSST    -7.826599e-01  6.434107542  12.359498501  12.151707840 1.763068e+01\nTbtm   -2.676387e-01  3.595118523   6.110801697   6.122372065 7.521761e+00\nU      -2.121380e-01 -0.010892980  -0.002634738  -0.010139401 7.229637e-04\nV      -1.883337e-01 -0.010722862  -0.002858645  -0.008474233 9.565173e-04\nXbtm    3.275602e-06  0.001458065   0.003088348   0.008360344 7.256525e-03\n               Max.  NA's\ndepth  4.964409e+03 59796\nMLD    1.066982e+02 59796\nSbtm   3.515742e+01 59796\nSSS    3.559161e+01 59796\nSST    2.643147e+01 59796\nTbtm   2.460999e+01 59796\nU      7.469980e-02 59796\nV      5.264002e-02 59796\nXbtm   1.899681e-01 59796\ndimension(s):\n      from  to offset    delta refsys point      values x/y\nx        1 121 -74.93  0.08226 WGS 84 FALSE        NULL [x]\ny        1  89  46.08 -0.08226 WGS 84 FALSE        NULL [y]\nmonth    1  12     NA       NA     NA    NA Jan,...,Dec    \n\n\nThis loads quite a complex set of arrays, but they have spatial information attached in the dimensions section. The x and y dimensions represent longitude and latitude respectively. The 3rd dimension, month, is time based.\nHere we plot all 12 months of sea surface temperature, SST. Note the they all share the same color scale so that they are easy to compare.\n\nplot(current['SST'])\n\n\n\n\n\n\n\n\nJust as we are able to plot linestrings/polygons along side points, we can also plot these with arrays (rasters). To do this for one month (“Apr”) of one variable (“SSS”) we simply need to slice that data out of the current variable.\n\napril_sss = current['SSS'] |&gt;\n  slice(\"month\", \"Apr\")\napril_sss\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n         Min. 1st Qu.   Median    Mean  3rd Qu.     Max. NA's\nSSS  16.44333 30.8342 31.10334 31.4641 31.93447 35.59161 4983\ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]\n\n\nThen it’s just plot, plot, plot.\n\nplot(april_sss, axes = TRUE, reset = FALSE)\nplot(st_geometry(coast), add = TRUE, col = \"orange\", lwd = 2)\nplot(st_geometry(buoys), add = TRUE, pch = 16, col = \"purple\")\n\n\n\n\n\n\n\n\nWe can plot ALL twelve months of a variable (“SST”) with the coast and points shown. There is one slight modification to be made since a single call to plot() actually gets invoked 12 times for this data. So where do we add in the buoys and coast? Fortunately, we can create what is called a “hook” function - who knows where the name hook came from? Once the hook function is defined, it will be applied to the each of the 12 subplots.\n\n# a little function that gets called just after each sub-plot\n# it simple adds the coast and buoy\nadd_coast_and_buoys = function(){\n  plot(st_geometry(coast), col = \"orange\", lwd = 2, add = TRUE)\n  plot(st_geometry(buoys), pch = 16, col = \"purple\", add = TRUE)\n}\n\n# here we call the plot, and tell R where to call `add_coast_and_buoys()` after\n# each subplot is made\nplot(current['SST'], hook = add_coast_and_buoys)",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C02_background.html",
    "href": "C02_background.html",
    "title": "Background",
    "section": "",
    "text": "No choice is the wrong choice as long as you make a choice. The only wrong choice is choosing not to make one. ~ Jake Abel\nTraditional ecological surveys are systematic, for a given species survey data sets tell us where the species is found and where it is absent. Using an observational data set (like OBIS) we only know where the species is found, which leaves us guessing about where they might not be found. This difference is what distinguishes a presence-abscence data set from a presence-only data set, and this difference guides the modeling process.\nWhen we model species distributions we are trying to define the environments where we should expect to find a species as well as the environments we would not expect to find a species. With OBIS data we have in hand the locations of observations, and we can extract the environmental data at those locations. To characterize the unoccupied environments we are going to have to sample what is called “background” (aka “background points” and “pseudo-absences”.)\nWe assume that we want the number of background points to be roughoy balanced with the number of observations. What balance means is open to interpretation, but if we have 100 observations then we woud like to have 50 to 200 background points in order to be balanced.\nWe want these background samples to roughly match the regional preferences of the observations; that is we want to avoid having observations that are mostly over Georges Bank while our background samples are primarily around the Bay of Fundy. We want there to be some reasonable proximity between occupied and unoccupied environments.\nHere we want to satisfy a couple of basic requirements…\nKeep in mind we will be glossing over important details; there is much more to investigate here, but tempus fugit and our course of study is brief. At the end we want to have in hand a set of locations that can be a companion for the obseravtions.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#thinning-observations",
    "href": "C02_background.html#thinning-observations",
    "title": "Background",
    "section": "1.1 Thinning observations",
    "text": "1.1 Thinning observations\nThinning is the process we use to sub-sample observations so that they are more evenly sampled across the spatial domain. We do this to discourage accidental sampling bias.\n\nthinned_aug_obs = thin_by_cell(aug_obs, mask)\nplot(mask, reset = FALSE, key.pos = NA, breaks = \"equal\")\nplot(coast, col = \"orange\", add = TRUE)\nplot(st_geometry(thinned_aug_obs), col = \"yellow\", cex = 0.1, add = TRUE)\n\n\n\n\n\n\n\n\nWe now have thinned the observations to just 1231 records. That has certainly made a difference - no more dense lines of observations even though we can discern the survey track lines.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#thinning-while-aggregating-array-cells",
    "href": "C02_background.html#thinning-while-aggregating-array-cells",
    "title": "Background",
    "section": "1.2 Thinning while aggregating array cells",
    "text": "1.2 Thinning while aggregating array cells\nBut we could pull another tool out of the bag of tricks - we could interpolate the underlying mask array cells so that it has larger cells. We do this by merging (aggregating) neighboring cells first, then thinning the observations. We’ll try aggregating by a factor of 2 - which means that two neighboring cells (in horizontal and in vertical) are merged. That is a 2x2 block of smaller cells becomes one bigger cell. An aggregating factor 3 means three cells in each direction are merged (a 3x3 block becomes 1 cell), and so on.\n\nwicked_thinned_aug_obs = thin_by_cell(aug_obs, mask, agg_fact = 2)\nplot(mask, reset = FALSE, key.pos = NA, breaks = \"equal\")\nplot(coast, col = \"orange\", add = TRUE)\nplot(st_geometry(wicked_thinned_aug_obs), col = \"yellow\", cex = 0.1, add = TRUE)\n\n\n\n\n\n\n\n\nWhoa, we now have thinned the observations to just 682 records. If you didn’t know that many of the observations came from straight-line surveys then you might not notice the faint structure of the distribution. Alas, you do have the knowledge so you probably can pick out the lines, but all in all this is a pretty good representation.\n\n\n\n\n\n\nNote\n\n\n\nHeads up! Some species might not need to thin the observations so aggressively. Some might not need us to aggregate the cells. Mola mola is a highly observed species during certain times of the year, so it merits aggressive thinning. Keep in mind you may need to treat your species observations differently - keep your options open!",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#randomly-sample-background-points",
    "href": "C02_background.html#randomly-sample-background-points",
    "title": "Background",
    "section": "1.3 Randomly sample background points",
    "text": "1.3 Randomly sample background points\nNow that we have a reasonable set of observation, we now need to choose points around those to represent the background. The sample_background() function requires three input arguments: the set of observations, the raster array and the number of points desired. We can also ask for the original presence observations to be returned. We’ll modify the output by adding a column identifying the month.\n\nobsbkg = sample_background(aug_obs, mask, 2*nrow(wicked_thinned_aug_obs),\n                           return_pres = TRUE) |&gt;\n  mutate(month = \"Aug\", .before = 1)    # .before tells R where to put this new column\nobsbkg\n\nSimple feature collection with 3884 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.89169 ymin: 38.805 xmax: -65.02004 ymax: 45.29627\nGeodetic CRS:  WGS 84\n# A tibble: 3,884 × 3\n   month class                geometry\n * &lt;chr&gt; &lt;fct&gt;             &lt;POINT [°]&gt;\n 1 Aug   presence   (-72.1214 39.8514)\n 2 Aug   presence       (-67.46 43.53)\n 3 Aug   presence (-72.35158 38.98323)\n 4 Aug   presence (-67.62971 43.99298)\n 5 Aug   presence   (-68.28717 41.857)\n 6 Aug   presence (-69.02174 40.98727)\n 7 Aug   presence   (-72.3179 40.5558)\n 8 Aug   presence (-67.80601 43.05127)\n 9 Aug   presence       (-68.23 42.74)\n10 Aug   presence (-68.19587 43.84306)\n# ℹ 3,874 more rows\n\n\nLet’s see where these fall relative to each other.\n\nplot(mask, reset = FALSE, key.pos = NA, breaks = \"equal\")\nplot(coast, col = \"orange\", add = TRUE)\nplot(obsbkg |&gt; filter(class == \"presence\") |&gt; st_geometry(), \n     col = \"yellow\", cex = 0.1, add = TRUE)\nplot(obsbkg |&gt; filter(class == \"background\") |&gt; st_geometry(), \n     col = \"blue\", cex = 0.2, add = TRUE)\n\n\n\n\n\n\n\n\nThese would be the data locations we feed into the model. So is that reasonable solution? At first glance it seems to be, so we’ll choose this background sampling approach; aggressive thinning followed by random sampling while avoiding observations. We can’t know if another approach might be better until we actually start modeling.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#a-function-we-can-reuse",
    "href": "C02_background.html#a-function-we-can-reuse",
    "title": "Background",
    "section": "2.1 A function we can reuse",
    "text": "2.1 A function we can reuse\nHere we make a function that needs at least three arguments: the complete set of observations, the mask used for thinning (possibly agressively) and sampling, as well as the month to filter the observations. The pseudo-code might look like this…\nfor a given month\n  filter the obs for that month\n  thin the obs (possibly agressively)\n  sample the background\n  return the combined thined observations/random background\nPhew! That’s a lot of steps. To manually run those steps 12 times would be tedious, so we roll that into a function that we can reuse 12 times instead.\nThis function will have a name, make_model_input_by_month. It’s a long name, but it makes it obvious what it does. First we start with the documentation.\n\n#' Builds a model input data set for a given month\n#' \n#' @param mon str, the month abbreviation for the month of interest (\"Jan\" by default)\n#' @param obs table, the complete observation data set\n#' @param raster stars, the object that defines the sampling space, usually a mask\n#' @param species str, the name of the species prepended to the name of the output files.\n#'   (By default \"Mola mola\" which gets converted to \"Mola_mola\")\n#' @param path the output data path to store this data (be default \"model_input\")\n#' @param agg_fact num, aggregation factor for aggressive thinning.  By default is 1 which means no aggregation.  For very dense observations, try a value of 2 to thin more aggressively.\n#' @param obs_to_bkg num, the ratio of observations to background\n#' @param min_obs num, this sets a threshold below which we wont try to make a model. (Default is 10)\n#' @return a named two element list of greedy and conservative model inputs - they are tables\nmake_model_input_by_month  = function(mon = \"Jan\",\n                                      obs = read_observations(\"Mola mola\"),\n                                      raster = NULL,\n                                      obs_to_bkg = 2, \n                                      species = \"Mola mola\",\n                                      path = data_path(\"model_input\"),\n                                      agg_fact = 1, \n                                      min_obs = 10){\n  # the user *must* provide a raster\n  if (is.null(raster)) stop(\"please provide a raster\")\n  \n  # filter the obs\n  obs = obs |&gt;\n    filter(month == mon[1])\n  \n  # check that we have at least some records, if not enough then alert the user\n  # and return NULL\n  if (nrow(obs) &lt; min_obs){\n    warning(\"sorry, this month has too few observations: \", mon)\n    return(NULL)\n  }\n  \n  # make sure the output path exists, if not, make it\n  path = make_path(path)\n  \n  thinned = thin_by_cell(obs, raster, agg_fact = agg_fact)\n  \n  # make the greedy model input by sampling the background\n  model_input = sample_background(thinned, raster, nrow(thinned) * obs_to_bkg,\n                                   return_pres = TRUE) |&gt;\n    mutate(month = mon, .before = 1)\n  # save the data\n  filename = sprintf(\"%s-%s-model_input.gpkg\", \n                     gsub(\" \", \"_\", species),\n                     mon)\n  write_sf(model_input, file.path(path, filename))\n  \n  # return, but disable automatic printing\n  invisible(model_input)\n}",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C03_covariates.html",
    "href": "C03_covariates.html",
    "title": "Covariates",
    "section": "",
    "text": "“In the end that was the choice you made, and it doesn’t matter how hard it was to make it. It matters that you did.”\n\nCassandra Clare\nNow we turn our attention to what we know and guess about the environments. We are using the Brickman data to make habitat suitability maps for select species under two climate scenarios (RCP45 and RCP85) at two different times (2055 and 2075) in the future. Each variable we might use is called covariate or predictor. Our covariates are nicely packaged up and tidy, but the reality is that it often requires a good deal of data wrangling if the data are messy.\nOur step here is to make sure that two or more covariates are not highly correlated if they are, then we would likely want to drop all but one.",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#reading-in-the-covariates",
    "href": "C03_covariates.html#reading-in-the-covariates",
    "title": "Covariates",
    "section": "2.1 Reading in the covariates",
    "text": "2.1 Reading in the covariates\nWe’ll read in the Brickman database, then filter two different subsets to read: “STATIC” covariate bathymetry that apply across all scenarios and times and monthly covariates for the “PRESENT” period. Note that depth is automatically included - that’s an option - see ?read_brickman for more information.\n\ndb = brickman_database()\npresent = read_brickman(filter(db, scenario == \"PRESENT\", interval == \"mon\"))\n\nWe have used August before as our example, let’s continue with August.\n\naug = present |&gt;\n  dplyr::slice(\"month\", \"Aug\")",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#make-a-pairs-plot",
    "href": "C03_covariates.html#make-a-pairs-plot",
    "title": "Covariates",
    "section": "2.2 Make a pairs plot",
    "text": "2.2 Make a pairs plot\nA pairs plot is a plot often used in exploratory data analysis. It makes a grid of mini-plots of a set of variables, and reveals the relationships among the variables pair-by-pair. It’s easy to make.\n\npairs(aug)\n\n\n\n\n\n\n\n\nIn the lower left portion of the plot we see paired scatter plots, at upper right we see the correlation values of the pairs, and long the diagonal we see a histogram of each variable. Some pairs are highly correlated, say over 0.7, and to include both in the modeling might not provide us with greater predictive power. It may feel counterintuitive to remove any variables - more data means more information, right? And more information means more informed models. Consider two measurements, human arm length and inseam. We might use these to predict if a person is tall, but since they are probably strongly collinear/correlated do we really need both?",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#identify-the-most-independent-variables-and-the-most-collinear",
    "href": "C03_covariates.html#identify-the-most-independent-variables-and-the-most-collinear",
    "title": "Covariates",
    "section": "2.3 Identify the most independent variables (and the most collinear)",
    "text": "2.3 Identify the most independent variables (and the most collinear)\nWe have a function that can help use select which variables to remove. filter_collinear() returns a listing of variables it suggests we keep. It attaches to the return value an attribute (like a post-it note stuck on a box) that lists the complementary variables that it suggests we drop. We are choosing a particular method, but you can learn more about using R’s help for ?filter_collinear.\n\nkeep = filter_collinear(aug, method = \"vif_step\")\nkeep\n\n[1] \"MLD\"  \"Sbtm\" \"SSS\"  \"SST\"  \"Tbtm\" \"U\"    \"V\"   \nattr(,\"to_remove\")\n[1] \"depth\" \"Xbtm\" \n\n\nOf course, we can decide to ignore this advice, and pick which ever ones we want including keeping them all. In fact, marine ecologists are loathe to drop depth; in coastal science in particular depth plays a significant role in ecology and biology. So, despite the high collinearity, we are going to force the use of depth.\n\nkeep = c(\"depth\", keep)\n\nWhatever selection of variables we decide to model with, we will save this listing to a file. That way we can refer to it programmatically, but that comes later.",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#a-closer-look-at-the-model-input-data",
    "href": "C03_covariates.html#a-closer-look-at-the-model-input-data",
    "title": "Covariates",
    "section": "2.4 A closer look at the model input data",
    "text": "2.4 A closer look at the model input data\nBefore we do commit to a selection of variables, let’s turn our attention back to our presence-background points, and look at just those chosen values rather than at values drawn form across the entire domain. Let’s open the file that contains the “greedy” model input for August during the PRESENT climate scenario.\n\nmodel_input = read_model_input(scientificname = \"Mola mola\", \n                               mon = \"Aug\")\nmodel_input\n\nSimple feature collection with 2046 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.89169 ymin: 38.8797 xmax: -65.02004 ymax: 45.29627\nGeodetic CRS:  WGS 84\n# A tibble: 2,046 × 3\n   month class                    geom\n   &lt;chr&gt; &lt;chr&gt;             &lt;POINT [°]&gt;\n 1 Aug   presence (-69.73939 43.00427)\n 2 Aug   presence (-68.84828 40.86383)\n 3 Aug   presence   (-71.7747 40.3501)\n 4 Aug   presence        (-66.3 41.07)\n 5 Aug   presence    (-71.837 40.2209)\n 6 Aug   presence (-72.78889 39.33647)\n 7 Aug   presence     (-69.845 39.791)\n 8 Aug   presence     (-68.175 42.088)\n 9 Aug   presence  (-69.1329 43.19357)\n10 Aug   presence    (-72.512 39.3369)\n# ℹ 2,036 more rows\n\n\nNext we’ll extract data values from our August covariates.\n\nvariables = extract_brickman(aug, model_input, form = \"wide\")\nvariables\n\n# A tibble: 2,046 × 10\n   point  depth   MLD  Sbtm   SSS   SST  Tbtm         U         V     Xbtm\n   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 p0001  163.   4.94  34.3  30.6  18.7  7.26  0.00496  -0.00360  0.00218 \n 2 p0002   68.6  4.43  33.1  30.8  20.8  6.44  0.00791  -0.0297   0.0114  \n 3 p0003   74.8  4.11  33.4  30.6  21.8  8.90 -0.00127  -0.00222  0.000908\n 4 p0004  898.   4.31  34.9  31.2  20.0  4.49 -0.0152   -0.0241   0.0105  \n 5 p0005   77.4  4.11  33.7  30.7  21.9  9.40 -0.00151  -0.00725  0.00263 \n 6 p0006   95.5  4.21  34.6  31.1  22.8 11.1   0.000985 -0.00287  0.00108 \n 7 p0007 1689.   5.68  35.0  31.9  22.6  3.84 -0.101    -0.00671  0.0510  \n 8 p0008  206.   4.87  34.7  30.7  19.6  7.41 -0.0115   -0.00125  0.00411 \n 9 p0009  173.   4.81  34.3  30.7  18.4  7.33 -0.00191  -0.000609 0.000710\n10 p0010  151.   4.64  35.1  31.3  22.9 10.4   0.00241  -0.00376  0.00158 \n# ℹ 2,036 more rows\n\n\nWe are going to call a plotting function, plot_pres_vs_bg(), that wants some of the data from model_input and some of the data in variables. So, we have to do some data wrangling to combine those; we’ll add class to variables and then drop the point column.\n\nvariables = variables |&gt;\n  mutate(class = model_input$class) |&gt;    # the $ extracts a column \n  select(-point)                          # the - means \"deselect\" or \"drop\"\nvariables\n\n# A tibble: 2,046 × 10\n    depth   MLD  Sbtm   SSS   SST  Tbtm         U         V     Xbtm class   \n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n 1  163.   4.94  34.3  30.6  18.7  7.26  0.00496  -0.00360  0.00218  presence\n 2   68.6  4.43  33.1  30.8  20.8  6.44  0.00791  -0.0297   0.0114   presence\n 3   74.8  4.11  33.4  30.6  21.8  8.90 -0.00127  -0.00222  0.000908 presence\n 4  898.   4.31  34.9  31.2  20.0  4.49 -0.0152   -0.0241   0.0105   presence\n 5   77.4  4.11  33.7  30.7  21.9  9.40 -0.00151  -0.00725  0.00263  presence\n 6   95.5  4.21  34.6  31.1  22.8 11.1   0.000985 -0.00287  0.00108  presence\n 7 1689.   5.68  35.0  31.9  22.6  3.84 -0.101    -0.00671  0.0510   presence\n 8  206.   4.87  34.7  30.7  19.6  7.41 -0.0115   -0.00125  0.00411  presence\n 9  173.   4.81  34.3  30.7  18.4  7.33 -0.00191  -0.000609 0.000710 presence\n10  151.   4.64  35.1  31.3  22.9 10.4   0.00241  -0.00376  0.00158  presence\n# ℹ 2,036 more rows\n\n\nFinally, can make a specialized plot comparing our variables for each class: presence and background.\n\nplot_pres_vs_bg(variables, \"class\")\n\n\n\n\n\n\n\n\nHow does this inform our thinking about reducing the number of variables? For which variables do presence and background values mirror each other? Which have the least overlap? We know that the model works by finding optimal combinations of covariates for the species. If there is never a difference between the conditions for presences and background then how will it find the optimal niche conditions?",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#saving-a-file-to-keep-track-of-modeling-choices",
    "href": "C03_covariates.html#saving-a-file-to-keep-track-of-modeling-choices",
    "title": "Covariates",
    "section": "2.5 Saving a file to keep track of modeling choices",
    "text": "2.5 Saving a file to keep track of modeling choices\nYou may have noticed that we write a lot of things to files (aka, “writing to disk”). It’s a useful practice especially when working with a multi-step process. One particular file, a configuration file, is used frequently in data science to store information about the choices we make as we work through our project. Configuration files generally are simple text files that we can easily get the computer to read and write.\nIn R, a confguration is treated as a named list. Each element of a list is named, but beyond that there aren’t any particular rules about confugurations. You can learn more about configurations in this tutorial.\nLet’s make a configuration list that holds 4 items: version identifier, species name and the names of the variables to model with.\n\ncfg = list(\n  version = \"Aug\",\n  scientificname = \"Mola mola\",\n  mon = \"Aug\",\n  keep_vars =  keep)\n\nWe can access by name three ways using what is called “indexing” : using the [[ indexing brackets, using the $ indexing operator or using the getElement() function.\n\ncfg[['scientificname']]\n\n[1] \"Mola mola\"\n\ncfg[[2]]\n\n[1] \"Mola mola\"\n\ncfg$scientificname\n\n[1] \"Mola mola\"\n\ngetElement(cfg, \"scientificname\")\n\n[1] \"Mola mola\"\n\ngetElement(cfg, 2)\n\n[1] \"Mola mola\"\n\n\nNow we’ll write this list to a file. First let’s set up a path where we might store these configurations, and for that matter, to store our modeling files. We’ll make a new directory, models/Aug and write the configuration there. We’ll use the famous “YAML” format to store the file. See the file functions/configuration.R for documentation on reading and writing.\n\nok = make_path(data_path(\"models\")) # make a directory for models\nwrite_configuration(cfg)            \n\nUse the Files pane to navigate to your personal data directory. Open the Aug.yaml file - this is what you configuration looks like in YAML. Fortunately we don’t mess by hand with these much.\nversion: Aug\nscientificname: Mola mola\nmon: Aug\nkeep_vars:\n- MLD\n- Sbtm\n- SSS\n- SST\n- Tbtm\n- U\n- V",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#challenge-c03",
    "href": "C03_covariates.html#challenge-c03",
    "title": "Covariates",
    "section": "4.1 Challenge C03",
    "text": "4.1 Challenge C03\nWrite a function that will read all of your configurations, and then generate a listing of the variables common to every model and a second listing of the ones that are not. Hint: see the help for intersect() and setdiff.",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C01_observations.html",
    "href": "C01_observations.html",
    "title": "Observations",
    "section": "",
    "text": "Follow this wiki page on obtaining data from OBIS. Keep in mind that you will probably want a species with sufficient number of records in the northwest Atlantic. Just what constitutes “sufficient” is probably subject to some debate, but a couple of hundred as a minumum will be helpful for learning. One thing that might help is to be on alert species that are only congregate in one area such as right along the shoreline or only appear in a few months of the year. It isn’t that those species are not worthy of study, but they may make the learning process harder.\nYou should feel free to get the data for a couple of different species, if one becomes a headache with our given resources, then you can switch easily to another.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#basisofrecord",
    "href": "C01_observations.html#basisofrecord",
    "title": "Observations",
    "section": "5.1 basisOfRecord",
    "text": "5.1 basisOfRecord\nNext we should examine the basisOfRecord variable to get an understanding of how these observations were made.\n\nobs |&gt; count(basisOfRecord)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -74.70772 ymin: 38.8 xmax: -65.00391 ymax: 45.1333\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  basisOfRecord              n                                              geom\n* &lt;chr&gt;                  &lt;int&gt;                                    &lt;GEOMETRY [°]&gt;\n1 HumanObservation        9491 MULTIPOINT ((-65.07 42.68), (-65.067 42.65), (-6…\n2 NomenclaturalChecklist     1                        POINT (-65.80602 44.97985)\n3 Occurrence                 1                          POINT (-65.2852 42.6243)\n4 PreservedSpecimen        170 MULTIPOINT ((-67.05534 45.09908), (-66.35 45.133…\n\n\nIf you are using a different species you may have different values for basisOfRecord. Let’s take a closer look at the complete records for one from each group.\n\nhuman = obs |&gt;\n  filter(basisOfRecord == \"HumanObservation\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/00026c58-b8a1-401c-9a1c-b457cc8ca1db\n\npreserved = obs |&gt;\n  filter(basisOfRecord == \"PreservedSpecimen\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/0093e48f-bef3-490a-9c24-10826c708baf\n\nchecklist = obs |&gt;\n  filter(basisOfRecord == \"NomenclaturalChecklist\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/c52e8172-146e-4298-bbc3-fb382097d308\n\noccurrence = obs |&gt;\n  filter(basisOfRecord == \"Occurrence\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/7f159a6a-9199-4985-8c99-17b30b04f75f\n\n\nNext let’s think about what our minimum requirements might be in order to build a model. To answer that we need to think about our environmental covariates in the Brickman data](https://github.com/BigelowLab/ColbyForecasting/wiki/Brickman). That data has dimensions of x (longitude), y (latitude) and month. In order to match obseravtions with that data, our observations must be complete in those three variables. Let’s take a look at a summary of the observations which will indicate the number of elements missing in each variable.\n\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9663        Length:9663        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2004-06-04   1st Qu.:2004  \n Mode  :character   Mode  :character   Median :2009-07-26   Median :2009  \n                                       Mean   :2006-12-23   Mean   :2006  \n                                       3rd Qu.:2016-11-18   3rd Qu.:2016  \n                                       Max.   :2024-01-03   Max.   :2024  \n                                       NA's   :7            NA's   :7     \n    month            eventTime         individualCount            geom     \n Length:9663        Length:9663        Min.   : 1.00   POINT        :9663  \n Class :character   Class :character   1st Qu.: 1.00   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.00   +proj=long...:   0  \n                                       Mean   : 1.11                       \n                                       3rd Qu.: 1.00                       \n                                       Max.   :25.00                       \n                                       NA's   :318",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#eventdate",
    "href": "C01_observations.html#eventdate",
    "title": "Observations",
    "section": "5.2 eventDate",
    "text": "5.2 eventDate\nFor Mola mola there are some rows where eventDate is NA. We need to filter those. The filter function looks for a vector of TRUE/FALSE values - one for each row. In our case, we test the eventDate column to see if it is NA, but then we reverse the TRUE/FALSE logical with the preceding ! (pronounded “bang!”). This we retain only the rows where eventDate is notNA`, and then we print the summary again.\n\nobs = obs |&gt;\n  filter(!is.na(eventDate))\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9656        Length:9656        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2004-06-04   1st Qu.:2004  \n Mode  :character   Mode  :character   Median :2009-07-26   Median :2009  \n                                       Mean   :2006-12-23   Mean   :2006  \n                                       3rd Qu.:2016-11-18   3rd Qu.:2016  \n                                       Max.   :2024-01-03   Max.   :2024  \n                                                                          \n    month            eventTime         individualCount            geom     \n Length:9656        Length:9656        Min.   : 1.00   POINT        :9656  \n Class :character   Class :character   1st Qu.: 1.00   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.00   +proj=long...:   0  \n                                       Mean   : 1.11                       \n                                       3rd Qu.: 1.00                       \n                                       Max.   :25.00                       \n                                       NA's   :315",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#individualcount",
    "href": "C01_observations.html#individualcount",
    "title": "Observations",
    "section": "5.3 individualCount",
    "text": "5.3 individualCount\nThat’s better, but we still have 315 NA values for individualCount. Let’s look at at least one record of those in detail; filter out one, and browse it.\n\nobs |&gt;\n  filter(is.na(individualCount)) |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/0027e671-6713-4299-9721-60834f0a5ae7\n\n\nWhat you find is, of course, dependent upon the species you have selected and any prior filtering steps you may have taken. Some may find that the observation is of a carcass, or possibly a keyboard entry error. How does one know without investigating each record. It quickly turns into a data quality investigation which can take a lot of time, but may be the most important step you take. In the interest of time for the class, we’ll simply filter those out of our observation dataset.\n\nobs = obs |&gt;\n  filter(!is.na(individualCount))\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9341        Length:9341        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-09-24   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-20   Median :2009  \n                                       Mean   :2006-11-10   Mean   :2006  \n                                       3rd Qu.:2016-11-23   3rd Qu.:2016  \n                                       Max.   :2024-01-03   Max.   :2024  \n    month            eventTime         individualCount            geom     \n Length:9341        Length:9341        Min.   : 1.00   POINT        :9341  \n Class :character   Class :character   1st Qu.: 1.00   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.00   +proj=long...:   0  \n                                       Mean   : 1.11                       \n                                       3rd Qu.: 1.00                       \n                                       Max.   :25.00                       \n\n\nWell now one has to wonder about a single observation of 25 animals. Let’s check that out.\n\nobs |&gt;\n  filter(individualCount == 25) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/cdf4363d-f954-4810-b15f-24334b0e6907\n\n\nOK, that seems like a legitimate source, and and it is possible, Mola mola can congregate for feeding, mating and possibly for karaoke parties.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#year",
    "href": "C01_observations.html#year",
    "title": "Observations",
    "section": "5.4 year",
    "text": "5.4 year\nWe know that the “current” climate scenario for the Brickman model data define “current” as the 1982-2013 window. It’s just an average, and if you have values from 1970 to the current year, you probably are safe in including them. But do your observations fall into those years? Let’s make a plot of the counts per year, with dashed lines shown the Brickman “current” cliamtology period.\n\nggplot(data = obs,\n       mapping = aes(x = year)) + \n  geom_bar() + \n  geom_vline(xintercept = c(1982, 2013), linetype = \"dashed\") + \n  labs(title = \"Counts per year\")\n\n\n\n\n\n\n\n\nFor this species, it seem like it is only the record from 1932 that might be a stretch, so let’s filter that out by rejecting records before 1970. This time, instead of asking for a sumamry, we’ll print the dimensions (rows, columns) of the table.\n\nobs = obs |&gt;\n  filter(year &gt;= 1970)\ndim(obs)\n\n[1] 9340    8\n\n\nThat’s still a lot of records. Now let’s check out the distribution across the months of the year.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#month",
    "href": "C01_observations.html#month",
    "title": "Observations",
    "section": "5.5 month",
    "text": "5.5 month\nWe will be making models and predictions for each month of the for the 4 future projection climates. Species and observers do show some seasonality, but it that seasonality so extreme that it might be impossible to model some months because of sparse data? Let’s make a plot of the counts per month.\n\nggplot(data = obs,\n       mapping = aes(x = month)) + \n  geom_bar() + \n  labs(title = \"Counts per month\")\n\n\n\n\n\n\n\n\nOh, rats! By default ggplot plots in alpha-numeric order, which scrambles our month order. To fix that we have to convert the month in a factor type while specifying the order of the factors, and we’ll use the mutate() function to help us. You can learn more about factors in this blog post\n\nobs = obs |&gt;\n  mutate(month = factor(month, levels = month.abb))\n\nggplot(data = obs,\n       mapping = aes(x = month)) + \n  geom_bar() + \n  labs(title = \"Counts per month\")\n\n\n\n\n\n\n\n\nThat’s better! So, it may be the for Mola mola we might not be able to successfully model in the cold winter months. That’s good to keep in mind.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#geometry",
    "href": "C01_observations.html#geometry",
    "title": "Observations",
    "section": "5.6 geometry",
    "text": "5.6 geometry\nLast, but certainly not least, we should consider the possibility that some observations might be on shore. It happens! We already know that some records included fish that were washed up on shore. It’s possible someone mis-keyed the longitude or latitude when entering the vaklues into the database. It’s alos possible that some observations fall just outside the areas where the Brickman data has values. To look for these points, we’ll load the Brickman mask (defines land vs water. Well, really it defines data vs no-data), and use that for further filtering.\nWe need to load the Brickman database, and then filter it for the static variable called “mask”.\n\ndb = brickman_database() |&gt;\n  filter(scenario == \"STATIC\", var == \"mask\")\nmask = read_brickman(db, add_depth = FALSE)\nmask\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n      Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\nmask     1       1      1    1       1    1 4983\ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]\n\n\nLet’s see what our mask looks like with the observations drizzled on top. Because the mask only has values of 1 (data) or NA (no-data). You’ll note that we only want to plot the locations of the observations, so we strip obs of everyhting except its geometery.\n\nplot(mask, breaks = \"equal\", axes = TRUE, reset = FALSE)\nplot(st_geometry(obs), pch = \".\", add = TRUE)\n\n\n\n\n\n\n\n\nMaybe with proper with squinting we can see some that fall into no-data areas. The sure-fire way to tell is to extract the mask values at the point locations.\n\nhitOrMiss = extract_brickman(mask, obs)\nhitOrMiss\n\n# A tibble: 9,340 × 3\n   point name  value\n   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 p0001 mask      1\n 2 p0002 mask      1\n 3 p0003 mask      1\n 4 p0004 mask      1\n 5 p0005 mask      1\n 6 p0006 mask      1\n 7 p0007 mask      1\n 8 p0008 mask      1\n 9 p0009 mask      1\n10 p0010 mask      1\n# ℹ 9,330 more rows\n\n\nOK, let’s tally the “value” variable.\n\ncount(hitOrMiss, value)\n\n# A tibble: 2 × 2\n  value     n\n  &lt;dbl&gt; &lt;int&gt;\n1     1  9307\n2    NA    33\n\n\nOoooo, a number of records in obs don’t line up with values in the mask (or in any Brickman data). We should filter those out; we’ll do so with a filter(). Note that we are reaching into the hitOrMiss table to access the value column when we use this hitOrMiss$value. Let’s figure out how many records we have dropped with all of this filtering.\n\ndim_start = dim(obs)\nobs = obs |&gt;\n  filter(!is.na(hitOrMiss$value))\ndim_end = dim(obs)\n\ndropped_records = dim_start[1] - dim_end[1]\ndropped_records\n\n[1] 33\n\n\nSo, we dropped 33 records which is about 0.4% of the raw OBIS data. Is it worth all that to drop just 4% of the data? Yes! Models are like all things computer… if you put garbage in you should expect to get garbage back out.",
    "crumbs": [
      "Observations"
    ]
  }
]